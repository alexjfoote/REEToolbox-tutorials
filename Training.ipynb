{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training.ipynb","provenance":[],"authorship_tag":"ABX9TyO33Im47MfvTwCxdSfh0SRm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Training Robust Models\n","\n","This notebook will demonstrate how you can include adversarial transforms during training to train more robust and potentially more accurate models.\n","\n","The basic process is very simple - we create our desired training loop as normal, and then add an adversarial transform to transform a batch of data before we train on it. This is very similar to standard data augmentation, but the adversarial optimisation finds more challenging transformations, resulting in greater improvements in robustness."],"metadata":{"id":"vYfMJP-5v6ro"}},{"cell_type":"markdown","source":["If you haven't already, make a copy of this Tutorials directory and a put it in the directory you want to work in.\n","\n","You must set the variable PATH to the directory containing this file."],"metadata":{"id":"w-YWyZDIwXmF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yJyGmjivFg1"},"outputs":[],"source":["PATH = \"\""]},{"cell_type":"markdown","source":["Import some useful functions"],"metadata":{"id":"-9eS-aOUgzue"}},{"cell_type":"code","source":["from reet.utils import load_resnet, load_pannuke, get_dataloader"],"metadata":{"id":"eZzLHgi6gyJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the device we're using and the class names"],"metadata":{"id":"GCjXy6qtliub"}},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","classes = [\"Negative\", \"Positive\"]"],"metadata":{"id":"_Ge-m5a_lf46"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the PanNuke dataset (see https://jgamper.github.io/PanNukeDataset/,  J. Gamper, N. A. Koohbanani, K. Benet, A. Khuram, and N. Rajpoot,\n","“PanNuke: An Open Pan-Cancer Histology Dataset for Nuclei Instance\n","Segmentation and Classification,” in Digital Pathology, pp. 11–19,\n","Springer, Cham, Apr. 2019.)"],"metadata":{"id":"mKUNP8gYhNi0"}},{"cell_type":"code","source":["data_path = os.path.join(PATH, \"Data/breast_folds.npz\")\n","Xtr, ytr, Xts, yts = load_pannuke(data_path)"],"metadata":{"id":"PkYwv15Xgvkw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we split our training data into a train and validation set and create a dictionary containing a training and validation data loader. We also create a dataloader from our test set."],"metadata":{"id":"zv7GKotFxCMV"}},{"cell_type":"code","source":["# Create dataloaders for the training loop\n","batch_size = 16\n","val_batch_size = batch_size\n","\n","# Number of epochs to train for\n","num_epochs = 15\n","\n","length = int(0.7 * len(Xtr))\n","\n","train_Xtr = Xtr[:length]\n","val_Xtr = Xtr[length:]\n","train_ytr = ytr[:length]\n","val_ytr = ytr[length:]\n","\n","train_data = torch.utils.data.TensorDataset(train_Xtr, train_ytr)\n","val_data = torch.utils.data.TensorDataset(val_Xtr, val_ytr)\n","\n","test_data = torch.utils.data.TensorDataset(Xts, yts)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n","\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","num_workers = 2\n","\n","dataloaders_dict = {\n","    \"train\": torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n","    \"val\": torch.utils.data.DataLoader(val_data, batch_size=val_batch_size, shuffle=True, num_workers=num_workers)\n","}"],"metadata":{"id":"BFY_nrUFxBJ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We load a pretrained model that we will fine-tune on our dataset and replace the final layer to fit our application (2 output neurons). We then gather all the parameters to update and create an optimiser for use during training."],"metadata":{"id":"p6PnaPnFzgig"}},{"cell_type":"code","source":["from torchvision import models\n","\n","def load_model():\n","    model = models.resnet18(pretrained=True)  \n","    model.fc = nn.Sequential(nn.Linear(512, n_classes),        \n","                            nn.LogSoftmax(dim=1))\n","    model = model.to(device)\n","    model.train()  \n","\n","    params_to_update = model.parameters()\n","    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n","    return model, optimizer_ft"],"metadata":{"id":"OylwrJgkzV9k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a standard training loop, which also takes a function that takes a batch of inputs and returns a transformed batch, which we can use to wrap our adversarial transforms."],"metadata":{"id":"Sflabwo6423q"}},{"cell_type":"code","source":["import copy\n","\n","def train_loop(model, dataloaders, criterion, optimizer, device=\"cuda:0\", transform_func=None, **kwargs):\n","    acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    t_full = time.time()\n","\n","    train_loader = dataloaders[\"train\"]\n","    val_loader = dataloaders[\"val\"]\n","\n","    for epoch in range(epochs): \n","        print(f'Epoch {epoch + 1}/{epochs}')\n","        print('-' * 10)   \n","        t_epoch = time.time() \n","\n","        num_examples = 0\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        model.train()\n","\n","        for stage in dataloaders:            \n","            for batch_no, (inputs, labels) in enumerate(train_loader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)  \n","\n","                with torch.set_grad_enabled(True):\n","                    optimizer.zero_grad()    \n","\n","                    if stage == \"train\":\n","                        model.train()\n","\n","                        if transform_func is not None:\n","                            inputs = transform_func(model, inputs, labels, **kwargs)  \n","\n","                        outputs = model(inputs)  \n","                        _, preds = torch.max(outputs, 1)\n","\n","                        loss = criterion(outputs, labels)\n","                                            \n","                        loss.backward()\n","                        optimizer.step()\n","                    else:\n","                        model.eval()\n","                        outputs = model(inputs)  \n","                        _, preds = torch.max(outputs, 1)\n","\n","                    num_examples += len(preds)\n","                    running_corrects += torch.sum(preds == labels.data)\n","                    running_loss += loss.item()\n","\n","                epoch_loss = running_loss / batches_per_epoch\n","                epoch_acc = running_corrects.double() / num_examples\n","                print_update(stage, epoch_loss, epoch_acc)\n","\n","        print(f\"{round(time.time() - t_epoch, 2)}s\")        \n","        \n","        if epoch_acc_v >= best_acc:\n","            best_acc = epoch_acc_v\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        acc_history.append(np.array([epoch_acc, epoch_acc_v]))\n","\n","    print(f\"Took: {round(time.time() - t_full, 2)}s\")\n","    print(f'Best val Acc: {best_acc}')\n","    model.load_state_dict(best_model_wts)\n","    return model, np.array(acc_history)"],"metadata":{"id":"o4L7fIXCznl6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we import the desired transforms and corresponding optimisers and parameters, as well as the evaluator, and create a function that will measure the accuracy and robustness of a model to the stain transform."],"metadata":{"id":"ifCWQsKv_ri4"}},{"cell_type":"code","source":["from reet.transforms import StainTransform\n","from reet.optimisers import PGD\n","from reet.constants import (eval_stain_transform_params,\n","                            eval_stain_optimiser_params)\n","from reet.evaluator import Evaluator\n","\n","def evaluate(model, test_dataset, test_dataloader):\n","    stain_evaluator = Evaluator(model, test_dataset, test_dataloader, PGD, \n","                                StainTransform, eval_stain_optimiser_params, \n","                                eval_stain_transform_params, device=device)\n","    results = stain_evaluator.predict(adversarial=True)\n","    get_metrics(results)"],"metadata":{"id":"fEI1lDct5qft"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To start we train a model without any adversarial transforms to get a baseline."],"metadata":{"id":"ugCAU3CMABmF"}},{"cell_type":"code","source":["from torch import nn\n","\n","std_model, optimizer_ft = load_model()\n","std_model, _ = train_loop(std_model, dataloaders_dict, nn.CrossEntropyLoss(), optimizer_ft, \n","           device=device, transform_func=None)\n","evaluate(std_model, test_data, test_loader)"],"metadata":{"id":"B9F4_y7a4yz8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we import `apply_transforms` and the default parameters for creating a stain adversarial optimiser. \n","\n","We create a list of tuples containing the optimisers and their adversarial optimiser parameters. We then iterate over this list and use the parameters to set up each optimiser. \n","\n","We can give this list of adversarial optimisers to `apply_transforms` and it will sample `k` optimisers and sequentially transform the data using the sampled adversarial optimisers. \n","\n","We pass `apply_transforms`, `k`, and the list of adversarial optimisers to the training loop, which will use them to transform each batch of data."],"metadata":{"id":"VMetTTzTAIia"}},{"cell_type":"code","source":["from reet.trainer import apply_transforms\n","from reet.constants import stain_adv_opt_params\n","\n","adv_model, optimizer_ft = load_model()\n","\n","optimizers_and_params = [(PGD, stain_adv_opt_params)]\n","\n","all_adv_opts = []\n","for TransformOptimiser, adv_opt_params in optimizers_and_params:\n","    all_adv_opts.append(TransformOptimiser(adv_model, **adv_opt_params, device=device))\n","\n","adv_model, _ = train_loop(adv_model, dataloaders_dict, nn.CrossEntropyLoss(), optimizer_ft, \n","           device=device, transform_func=apply_transforms, k=1, \n","           adv_optimisers=all_adv_opts)\n","\n","evaluate(adv_model, test_data, test_loader)"],"metadata":{"id":"HIHi31vD6n_f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, including the adversarial stain transform improved the model's accuracy, and significantly improved it's robustness to the stain transform, compared to the model trained without the transform."],"metadata":{"id":"77EVYfEVBpq_"}},{"cell_type":"markdown","source":["You can also use the built in implementation of adversarial training for free (see https://arxiv.org/abs/1904.12843), adapted for use with multiple adversarial transforms. This reorders the data, repeating each `m` times in a row and performing the optimisation over the repetitions for more efficient adversarial training. This is beneficial when you want to a more expensive adversarial optimisation, such as multi-step PGD - you can do one step per repetition of the batch and reach comparable robustness with lower training times.\n","\n","You can also use the built in evaluation function, which will take a list of dictionaries that contain all the info needed to set up and run an evaluation, and perform all the evaluations."],"metadata":{"id":"lj_erLBXs0T6"}},{"cell_type":"code","source":["from reet.trainer import train, evaluation\n","\n","model_params = {\n","    \"path\": None,\n","    \"load_saved\": False,\n","    \"pretrained\": True,\n","    \"n_classes\": 2\n","}\n","\n","train_params = {\n","    \"dataloaders\": dataloaders_dict,\n","    \"criterion\": nn.CrossEntropyLoss(),\n","    \"initial_epochs\": 0,\n","    \"adv_epochs\": 30,\n","    \"m\": 5,\n","    \"last_n\": 30,\n","    \"k\": 1\n","}\n","\n","adv_free_model, optimizer_ft = load_model()\n","adv_free_model, _ = train(adv_free_model, optimizer_ft, optimizers_and_params, \n","                          train_params, device=device)\n","\n","stain_evaluator_params = {\n","    \"dataset\": test_data,\n","    \"dataloader\": test_loader,\n","    \"TransformOptimiser\": PGD,\n","    \"Transform\": StainTransform,\n","    \"optimiser_params\": eval_stain_optimiser_params,\n","    \"trans_params\": eval_stain_trans_params\n","}\n","\n","evaluator_params = [stain_evaluator_params]\n","\n","evaluation(adv_free_model, evaluator_params, device=device)"],"metadata":{"id":"uLv6ZE0jrwt-"},"execution_count":null,"outputs":[]}]}